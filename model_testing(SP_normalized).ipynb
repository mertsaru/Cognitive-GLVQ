{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import cognitive_GLVQ as cglvq\n",
    "import cognitive_GLVQ2 as cglvq2\n",
    "import GLVQ as glvq\n",
    "import optimizer as opt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary\n",
    "\n",
    "features : narray = (\n",
    "    feature 1,\n",
    "    feature 2,\n",
    "    ...\n",
    ")\n",
    "\n",
    "labels : narray = (\n",
    "    label 1,\n",
    "    label 2,\n",
    "    ...\n",
    ")\n",
    "\n",
    "classes = {\n",
    "    \"wo\": 0,\n",
    "    \"Ak\": 1,\n",
    "    \"Al\": 2,\n",
    "    \"Mk\": 3,\n",
    "    \"Ml\": 4,\n",
    "    \"Gk\": 5,\n",
    "    \"Gl\": 6,\n",
    "    \"ud\": 7\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal vs. Abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_prototypes = 5\n",
    "feature_dir = \"dataset/feature/inputs_wo_ud_w_sp.npy\"\n",
    "label_dir = \"dataset/label/labels_wo_ud.npy\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load(feature_dir)\n",
    "features /= 255\n",
    "labels = np.load(label_dir)\n",
    "labels = labels.astype(int)\n",
    "labels = np.where(labels == 0, labels, 1) # Divides the labels into normal(0) and abnormal(1)\n",
    "\n",
    "# class counts\n",
    "unique, count = np.unique(labels, return_counts=True)\n",
    "dict(zip(unique, count))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prototype selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prototype selection\n",
    "index_normal = np.where(labels == 0)\n",
    "index_normal = list(index_normal[0])\n",
    "index_abnormal = np.where(labels == 1)\n",
    "index_abnormal = list(index_abnormal[0])\n",
    "\n",
    "## Random prototype selection\n",
    "prototype_normal_index = random.sample(index_normal, num_prototypes)\n",
    "prototype_abnormal_index = random.sample(index_abnormal, num_prototypes)\n",
    "prototypes_index = prototype_normal_index + prototype_abnormal_index\n",
    "prototypes_index.sort(reverse=True)\n",
    "\n",
    "prototypes = [(features[i],labels[i]) for i in prototypes_index]\n",
    "\n",
    "## Remove prototypes from dataset\n",
    "features = np.delete(features, prototypes_index, axis=0)\n",
    "labels = np.delete(labels, prototypes_index, axis=0)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_normal = np.where(labels == 0)\n",
    "index_normal = list(index_normal[0])\n",
    "index_abnormal = np.where(labels == 1)\n",
    "index_abnormal = list(index_abnormal[0])\n",
    "\n",
    "# Select equal amount of normal and abnormal data\n",
    "data_per_class = min(count) - num_prototypes\n",
    "index_normal_sample = random.sample(index_normal, data_per_class)\n",
    "dataset_index = index_normal_sample + index_abnormal\n",
    "random.shuffle(dataset_index)\n",
    "dataset_acc = [(features[i],labels[i]) for i in dataset_index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split test, training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 15% of the dataset for testing\n",
    "len_dataset_acc = len(dataset_acc)\n",
    "test_size = 0.15\n",
    "test_set_acc = dataset_acc[:int(len_dataset_acc*test_size)]\n",
    "train_set_acc = dataset_acc[int(len_dataset_acc*test_size):]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### GLVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.3\n",
    "glvq_acc = glvq.GLVQ(prototypes, learning_rate)\n",
    "hist = glvq_acc.train(num_epochs, train_set_acc, test_set_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glvq_acc.lr_graph(\"glvq_acc\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_acc = cglvq.CGLVQ(prototypes)\n",
    "hist = cp_glvq_acc.train(num_epochs, train_set_acc, test_set_acc, opt.conditional_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_acc.lr_graph(\"cp_acc\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_acc2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = cp_glvq_acc2.train(num_epochs, train_set_acc, test_set_acc, opt.conditional_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_acc2.lr_graph(\"cp_acc2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_acc = cglvq.CGLVQ(prototypes)\n",
    "hist = ls_glvq_acc.train(num_epochs, train_set_acc, test_set_acc, opt.loose_symmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_acc.lr_graph(\"ls_acc\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_acc2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = ls_glvq_acc2.train(num_epochs, train_set_acc, test_set_acc, opt.loose_symmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_acc2.lr_graph(\"ls_acc2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_acc = cglvq.CGLVQ(prototypes)\n",
    "hist = lsr_glvq_acc.train(num_epochs, train_set_acc, test_set_acc, opt.loose_symmetry_rarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_acc.lr_graph(\"lsr_acc\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSR 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_acc2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = lsr_glvq_acc2.train(num_epochs, train_set_acc, test_set_acc, opt.loose_symmetry_rarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_acc2.lr_graph(\"lsr_acc2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DFH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_acc = cglvq.CGLVQ(prototypes)\n",
    "hist = dfh_glvq_acc.train(num_epochs, train_set_acc, test_set_acc, opt.dual_factor_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_acc.lr_graph(\"dfh_acc\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DFH 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_acc2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = dfh_glvq_acc2.train(num_epochs, train_set_acc, test_set_acc, opt.dual_factor_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_acc2.lr_graph(\"dfh_acc2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dataset_f1 = len(features)\n",
    "dataset_f1 = [(features[i],labels[i]) for i in range(len_dataset_f1)]\n",
    "random.shuffle(dataset_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split test, training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 15% of the dataset for testing\n",
    "test_size = 0.15\n",
    "test_set_f1 = dataset_f1[:int(len_dataset_f1*test_size)]\n",
    "train_set_f1 = dataset_f1[int(len_dataset_f1*test_size):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### GLVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.3\n",
    "glvq_f1 = glvq.GLVQ(prototypes, learning_rate)\n",
    "hist = glvq_f1.train(num_epochs, train_set_f1, test_set_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glvq_f1.lr_graph(\"glvq_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_f1 = cglvq.CGLVQ(prototypes)\n",
    "hist = cp_glvq_f1.train(num_epochs, train_set_f1, test_set_f1, opt.conditional_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_f1.lr_graph(\"cp_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_f1_2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = cp_glvq_f1_2.train(num_epochs, train_set_f1, test_set_f1, opt.conditional_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_f1_2.lr_graph(\"cp_f1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_f1 = cglvq.CGLVQ(prototypes)\n",
    "hist = ls_glvq_f1.train(num_epochs, train_set_f1, test_set_f1, opt.loose_symmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_f1.lr_graph(\"ls_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_f1_2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = ls_glvq_f1_2.train(num_epochs, train_set_f1, test_set_f1, opt.loose_symmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_f1_2.lr_graph(\"ls_f1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_f1 = cglvq.CGLVQ(prototypes)\n",
    "hist = lsr_glvq_f1.train(num_epochs, train_set_f1, test_set_f1, opt.loose_symmetry_rarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_f1.lr_graph(\"lsr_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSR 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_f1_2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = lsr_glvq_f1_2.train(num_epochs, train_set_f1, test_set_f1, opt.loose_symmetry_rarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_f1_2.lr_graph(\"lsr_f1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DFH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_f1 = cglvq.CGLVQ(prototypes)\n",
    "hist = dfh_glvq_f1.train(num_epochs, train_set_f1, test_set_f1, opt.dual_factor_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_f1.lr_graph(\"dfh_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DFH 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_f1_2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = dfh_glvq_f1_2.train(num_epochs, train_set_f1, test_set_f1, opt.dual_factor_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_f1_2.lr_graph(\"dfh_f1_2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_prototypes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load(feature_dir)\n",
    "labels = np.load(label_dir)\n",
    "labels = labels.astype(int)\n",
    "\n",
    "# class counts\n",
    "unique, count = np.unique(labels, return_counts=True)\n",
    "dict(zip(unique, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Prototype selection\n",
    "prototypes_index = []\n",
    "for i in range(7):\n",
    "    index_i = np.where(labels == i)\n",
    "    index_i = list(index_i[0])\n",
    "    prototype_index_i = random.sample(index_i, num_prototypes)\n",
    "    prototypes_index += prototype_index_i\n",
    "prototypes_index.sort(reverse=True)\n",
    "\n",
    "prototypes = [(features[i],labels[i]) for i in prototypes_index]\n",
    "\n",
    "## Remove prototypes from dataset\n",
    "features = np.delete(features, prototypes_index, axis=0)\n",
    "labels = np.delete(labels, prototypes_index, axis=0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select equal amount of normal and abnormal data\n",
    "data_per_class = min(count) - num_prototypes\n",
    "\n",
    "dataset_index = []\n",
    "for i in range(7):\n",
    "    index_i = np.where(labels == i)\n",
    "    index_i = list(index_i[0])\n",
    "    index_i_sample = random.sample(index_i, data_per_class)\n",
    "    dataset_index += index_i_sample\n",
    "\n",
    "random.shuffle(dataset_index)\n",
    "dataset_acc = [(features[i],labels[i]) for i in dataset_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split test, training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 15% of the dataset for testing\n",
    "len_dataset_acc = len(dataset_acc)\n",
    "test_size = 0.15\n",
    "test_set_acc = dataset_acc[:int(len_dataset_acc*test_size)]\n",
    "train_set_acc = dataset_acc[int(len_dataset_acc*test_size):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### GLVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.3\n",
    "glvq_acc = glvq.GLVQ(prototypes, learning_rate)\n",
    "hist = glvq_acc.train(num_epochs, train_set_acc, test_set_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glvq_acc.lr_graph(\"glvq_acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_acc = cglvq.CGLVQ(prototypes)\n",
    "hist = cp_glvq_acc.train(num_epochs, train_set_acc, test_set_acc, opt.conditional_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_acc.lr_graph(\"cp_acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_acc2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = cp_glvq_acc2.train(num_epochs, train_set_acc, test_set_acc, opt.conditional_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_acc2.lr_graph(\"cp_acc2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_acc = cglvq.CGLVQ(prototypes)\n",
    "hist = ls_glvq_acc.train(num_epochs, train_set_acc, test_set_acc, opt.loose_symmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_acc.lr_graph(\"ls_acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_acc2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = ls_glvq_acc2.train(num_epochs, train_set_acc, test_set_acc, opt.loose_symmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_acc2.lr_graph(\"ls_acc2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_acc = cglvq.CGLVQ(prototypes)\n",
    "hist = lsr_glvq_acc.train(num_epochs, train_set_acc, test_set_acc, opt.loose_symmetry_rarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_acc.lr_graph(\"lsr_acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSR 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_acc2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = lsr_glvq_acc2.train(num_epochs, train_set_acc, test_set_acc, opt.loose_symmetry_rarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_acc2.lr_graph(\"lsr_acc2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DFH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_acc = cglvq.CGLVQ(prototypes)\n",
    "hist = dfh_glvq_acc.train(num_epochs, train_set_acc, test_set_acc, opt.dual_factor_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_acc.lr_graph(\"dfh_acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DFH 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_acc2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = dfh_glvq_acc2.train(num_epochs, train_set_acc, test_set_acc, opt.dual_factor_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_acc2.lr_graph(\"dfh_acc2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dataset_f1 = len(features)\n",
    "dataset_f1 = [(features[i],labels[i]) for i in range(len_dataset_f1)]\n",
    "random.shuffle(dataset_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split test, training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 15% of the dataset for testing\n",
    "test_size = 0.15\n",
    "test_set_f1 = dataset_f1[:int(len_dataset_f1*test_size)]\n",
    "train_set_f1 = dataset_f1[int(len_dataset_f1*test_size):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### GLVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.3\n",
    "glvq_f1 = glvq.GLVQ(prototypes, learning_rate)\n",
    "hist = glvq_f1.train(num_epochs, train_set_f1, test_set_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glvq_f1.lr_graph(\"glvq_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_f1 = cglvq.CGLVQ(prototypes)\n",
    "hist = cp_glvq_f1.train(num_epochs, train_set_f1, test_set_f1, opt.conditional_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_f1.lr_graph(\"cp_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_f1_2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = cp_glvq_f1_2.train(num_epochs, train_set_f1, test_set_f1, opt.conditional_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_f1_2.lr_graph(\"cp_f1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_f1 = cglvq.CGLVQ(prototypes)\n",
    "hist = ls_glvq_f1.train(num_epochs, train_set_f1, test_set_f1, opt.loose_symmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_f1.lr_graph(\"ls_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_f1_2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = ls_glvq_f1_2.train(num_epochs, train_set_f1, test_set_f1, opt.loose_symmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_f1_2.lr_graph(\"ls_f1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_f1 = cglvq.CGLVQ(prototypes)\n",
    "hist = lsr_glvq_f1.train(num_epochs, train_set_f1, test_set_f1, opt.loose_symmetry_rarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_f1.lr_graph(\"lsr_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSR 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_f1_2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = lsr_glvq_f1_2.train(num_epochs, train_set_f1, test_set_f1, opt.loose_symmetry_rarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_f1_2.lr_graph(\"lsr_f1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DFH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_f1 = cglvq.CGLVQ(prototypes)\n",
    "hist = dfh_glvq_f1.train(num_epochs, train_set_f1, test_set_f1, opt.dual_factor_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_f1.lr_graph(\"dfh_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DFH 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_f1_2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = dfh_glvq_f1_2.train(num_epochs, train_set_f1, test_set_f1, opt.dual_factor_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_f1_2.lr_graph(\"dfh_f1_2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only Sickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_prototypes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load(feature_dir)\n",
    "labels = np.load(label_dir)\n",
    "labels = labels.astype(int)\n",
    "\n",
    "index_normal = np.where(labels == 0)\n",
    "index_normal = list(index_normal[0])\n",
    "features = np.delete(features, index_normal, axis=0)\n",
    "labels = np.delete(labels, index_normal, axis=0)\n",
    "\n",
    "# class counts\n",
    "unique, count = np.unique(labels, return_counts=True)\n",
    "dict(zip(unique, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Random Prototype selection\n",
    "prototypes_index = []\n",
    "for i in range(1,7):\n",
    "    index_i = np.where(labels == i)\n",
    "    index_i = list(index_i[0])\n",
    "    prototype_index_i = random.sample(index_i, num_prototypes)\n",
    "    prototypes_index += prototype_index_i\n",
    "prototypes_index.sort(reverse=True)\n",
    "\n",
    "prototypes = [(features[i],labels[i]) for i in prototypes_index]\n",
    "\n",
    "## Remove prototypes from dataset\n",
    "features = np.delete(features, prototypes_index, axis=0)\n",
    "labels = np.delete(labels, prototypes_index, axis=0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select equal amount of normal and abnormal data\n",
    "data_per_class = min(count) - num_prototypes\n",
    "\n",
    "dataset_index = []\n",
    "for i in range(1,7):\n",
    "    index_i = np.where(labels == i)\n",
    "    index_i = list(index_i[0])\n",
    "    index_i_sample = random.sample(index_i, data_per_class)\n",
    "    dataset_index += index_i_sample\n",
    "\n",
    "random.shuffle(dataset_index)\n",
    "dataset_acc = [(features[i],labels[i]) for i in dataset_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split test, training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 15% of the dataset for testing\n",
    "len_dataset_acc = len(dataset_acc)\n",
    "test_size = 0.15\n",
    "test_set_acc = dataset_acc[:int(len_dataset_acc*test_size)]\n",
    "train_set_acc = dataset_acc[int(len_dataset_acc*test_size):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### GLVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.3\n",
    "glvq_acc = glvq.GLVQ(prototypes, learning_rate)\n",
    "hist = glvq_acc.train(num_epochs, train_set_acc, test_set_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glvq_acc.lr_graph(\"glvq_acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_acc = cglvq.CGLVQ(prototypes)\n",
    "hist = cp_glvq_acc.train(num_epochs, train_set_acc, test_set_acc, opt.conditional_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_acc.lr_graph(\"cp_acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_acc2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = cp_glvq_acc2.train(num_epochs, train_set_acc, test_set_acc, opt.conditional_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_acc2.lr_graph(\"cp_acc2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_acc = cglvq.CGLVQ(prototypes)\n",
    "hist = ls_glvq_acc.train(num_epochs, train_set_acc, test_set_acc, opt.loose_symmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_acc.lr_graph(\"ls_acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_acc2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = ls_glvq_acc2.train(num_epochs, train_set_acc, test_set_acc, opt.loose_symmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_acc2.lr_graph(\"ls_acc2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_acc = cglvq.CGLVQ(prototypes)\n",
    "hist = lsr_glvq_acc.train(num_epochs, train_set_acc, test_set_acc, opt.loose_symmetry_rarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_acc.lr_graph(\"lsr_acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSR 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_acc2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = lsr_glvq_acc2.train(num_epochs, train_set_acc, test_set_acc, opt.loose_symmetry_rarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_acc2.lr_graph(\"lsr_acc2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DFH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_acc = cglvq.CGLVQ(prototypes)\n",
    "hist = dfh_glvq_acc.train(num_epochs, train_set_acc, test_set_acc, opt.dual_factor_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_acc.lr_graph(\"dfh_acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DFH 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_acc2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = dfh_glvq_acc2.train(num_epochs, train_set_acc, test_set_acc, opt.dual_factor_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_acc2.lr_graph(\"dfh_acc2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dataset_f1 = len(features)\n",
    "dataset_f1 = [(features[i],labels[i]) for i in range(len_dataset_f1)]\n",
    "random.shuffle(dataset_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split test, training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 15% of the dataset for testing\n",
    "test_size = 0.15\n",
    "test_set_f1 = dataset_f1[:int(len_dataset_f1*test_size)]\n",
    "train_set_f1 = dataset_f1[int(len_dataset_f1*test_size):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### GLVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.3\n",
    "glvq_f1 = glvq.GLVQ(prototypes, learning_rate)\n",
    "hist = glvq_f1.train(num_epochs, train_set_f1, test_set_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glvq_f1.lr_graph(\"glvq_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_f1 = cglvq.CGLVQ(prototypes)\n",
    "hist = cp_glvq_f1.train(num_epochs, train_set_f1, test_set_f1, opt.conditional_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_f1.lr_graph(\"cp_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CP 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_f1_2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = cp_glvq_f1_2.train(num_epochs, train_set_f1, test_set_f1, opt.conditional_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_glvq_f1_2.lr_graph(\"cp_f1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_f1 = cglvq.CGLVQ(prototypes)\n",
    "hist = ls_glvq_f1.train(num_epochs, train_set_f1, test_set_f1, opt.loose_symmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_f1.lr_graph(\"ls_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_f1_2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = ls_glvq_f1_2.train(num_epochs, train_set_f1, test_set_f1, opt.loose_symmetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_glvq_f1_2.lr_graph(\"ls_f1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_f1 = cglvq.CGLVQ(prototypes)\n",
    "hist = lsr_glvq_f1.train(num_epochs, train_set_f1, test_set_f1, opt.loose_symmetry_rarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_f1.lr_graph(\"lsr_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LSR 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_f1_2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = lsr_glvq_f1_2.train(num_epochs, train_set_f1, test_set_f1, opt.loose_symmetry_rarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsr_glvq_f1_2.lr_graph(\"lsr_f1_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DFH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_f1 = cglvq.CGLVQ(prototypes)\n",
    "hist = dfh_glvq_f1.train(num_epochs, train_set_f1, test_set_f1, opt.dual_factor_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_f1.lr_graph(\"dfh_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DFH 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_f1_2 = cglvq2.CGLVQ(prototypes,0.3)\n",
    "hist = dfh_glvq_f1_2.train(num_epochs, train_set_f1, test_set_f1, opt.dual_factor_heuristic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh_glvq_f1_2.lr_graph(\"dfh_f1_2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
