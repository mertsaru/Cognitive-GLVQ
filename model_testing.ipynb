{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import cognitive_GLVQ as cglvq\n",
    "import optimizer as opt\n",
    "import GLVQ as glvq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary\n",
    "\n",
    "features : narray = (\n",
    "    feature 1,\n",
    "    feature 2,\n",
    "    ...\n",
    ")\n",
    "\n",
    "labels : narray = (\n",
    "    label 1,\n",
    "    label 2,\n",
    "    ...\n",
    ")\n",
    "\n",
    "classes = {\n",
    "    \"wo\": 0,\n",
    "    \"Ak\": 1,\n",
    "    \"Al\": 2,\n",
    "    \"Mk\": 3,\n",
    "    \"Ml\": 4,\n",
    "    \"Gk\": 5,\n",
    "    \"Gl\": 6,\n",
    "    \"ud\": 7\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_prototypes = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With SP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with fourier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal vs. Abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load(\"dataset/feature/inputs_wo_ud_w_sp_f.npy\")\n",
    "labels = np.load(\"dataset/label/labels_wo_ud.npy\")\n",
    "dataset_size = len(features)\n",
    "labels = np.where(labels == 0, labels, 1) # Divides the labels into normal(0) and abnormal(1)\n",
    "\n",
    "# Shuffle the dataset\n",
    "#permutation = np.random.permutation(dataset_size)\n",
    "#features = features[permutation]\n",
    "#labels = labels[permutation]\n",
    "\n",
    "unique, count = np.unique(labels, return_counts=True)\n",
    "dict(zip(unique, count))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prototype selection\n",
    "index_normal = np.where(labels == 0)\n",
    "index_normal = list(index_normal[0])\n",
    "index_abnormal = np.where(labels == 1)\n",
    "index_abnormal = list(index_abnormal[0])\n",
    "\n",
    "## Random prototype selection\n",
    "prototype_normal_index = random.sample(index_normal, num_prototypes)\n",
    "prototype_abnormal_index = random.sample(index_abnormal, num_prototypes)\n",
    "prototype_normal_index.sort(reverse=True)\n",
    "prototype_abnormal_index.sort(reverse=True)\n",
    "\n",
    "prototypes_index = prototype_normal_index + prototype_abnormal_index\n",
    "prototypes_index.sort(reverse=True)\n",
    "\n",
    "prototypes = [(features[i],labels[i]) for i in prototypes_index]\n",
    "\n",
    "## Remove prototypes from dataset\n",
    "features = np.delete(features, prototypes_index, axis=0)\n",
    "labels = np.delete(labels, prototypes_index, axis=0)    \n",
    "\n",
    "index_normal = np.where(labels == 0)\n",
    "index_normal = list(index_normal[0])\n",
    "index_abnormal = np.where(labels == 1)\n",
    "index_abnormal = list(index_abnormal[0])\n",
    "\n",
    "# Select equal amount of normal and abnormal data\n",
    "data_per_class = min(count) - num_prototypes\n",
    "index_normal_sample = random.sample(index_normal, data_per_class)\n",
    "dataset_index = index_normal_sample + index_abnormal\n",
    "\n",
    "#print(index_normal_sample)\n",
    "#print(index_abnormal)\n",
    "dataset_index.sort(reverse=True)\n",
    "\n",
    "dataset = [(features[i],labels[i]) for i in dataset_index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split test, training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 15% of the dataset for testing\n",
    "test_size = 0.15\n",
    "test_index = random.sample(range(len(dataset)), int(len(dataset) * test_size))\n",
    "test_set = [dataset[i] for i in test_index]\n",
    "training_set = [dataset[i] for i in range(len(dataset)) if i not in test_index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_f_na_glvq_cp = cglvq.CGLVQ(prototypes)\n",
    "sp_f_na_cp_hist = sp_f_na_glvq_cp.train(num_epochs, training_set, test_set, opt.conditional_probability,measure = \"test_measure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sp_f_na_glvq_ls = cglvq.CGLVQ(prototypes)\n",
    "#sp_f_na_ls_hist = sp_f_na_glvq_ls.train(num_epochs, training_set, test_set, opt.loose_symmetry,measure = \"test_measure\")\n",
    "for epoch in sp_f_na_ls_hist:\n",
    "    split = random.randint(0, len(epoch[\"prototypes\"][0][\"feature\"]) - 4)\n",
    "    print(\"feature: \",epoch[\"prototypes\"][0][\"feature\"][0:2])\n",
    "    print(f\"update : {(epoch['prototypes'][0]['update_sum'] * epoch['prototypes'][0]['lr'])[0:2]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Between Sick Groups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without fourier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal vs. Abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load(\"dataset/feature/inputs_wo_ud_w_sp.npy\")\n",
    "labels = np.load(\"dataset/label/labels_wo_ud.npy\")\n",
    "dataset_size = len(features)\n",
    "labels = np.where(labels == 0, labels, 1) # Divides the labels into normal(0) and abnormal(1)\n",
    "\n",
    "# Shuffle the dataset\n",
    "#permutation = np.random.permutation(dataset_size)\n",
    "#features = features[permutation]\n",
    "#labels = labels[permutation]\n",
    "\n",
    "unique, count = np.unique(labels, return_counts=True)\n",
    "dict(zip(unique, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prototype selection\n",
    "index_normal = np.where(labels == 0)\n",
    "index_normal = list(index_normal[0])\n",
    "index_abnormal = np.where(labels == 1)\n",
    "index_abnormal = list(index_abnormal[0])\n",
    "\n",
    "## Random prototype selection\n",
    "prototype_normal_index = random.sample(index_normal, num_prototypes)\n",
    "prototype_abnormal_index = random.sample(index_abnormal, num_prototypes)\n",
    "prototype_normal_index.sort(reverse=True)\n",
    "prototype_abnormal_index.sort(reverse=True)\n",
    "\n",
    "prototypes_index = prototype_normal_index + prototype_abnormal_index\n",
    "prototypes_index.sort(reverse=True)\n",
    "\n",
    "prototypes = [(features[i],labels[i]) for i in prototypes_index]\n",
    "\n",
    "## Remove prototypes from dataset\n",
    "features = np.delete(features, prototypes_index, axis=0)\n",
    "labels = np.delete(labels, prototypes_index, axis=0)    \n",
    "\n",
    "index_normal = np.where(labels == 0)\n",
    "index_normal = list(index_normal[0])\n",
    "index_abnormal = np.where(labels == 1)\n",
    "index_abnormal = list(index_abnormal[0])\n",
    "\n",
    "# Select equal amount of normal and abnormal data\n",
    "data_per_class = min(count) - num_prototypes\n",
    "index_normal_sample = random.sample(index_normal, data_per_class)\n",
    "dataset_index = index_normal_sample + index_abnormal\n",
    "\n",
    "#print(index_normal_sample)\n",
    "#print(index_abnormal)\n",
    "dataset_index.sort(reverse=True)\n",
    "\n",
    "dataset = [(features[i],labels[i]) for i in dataset_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split test, training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 15% of the dataset for testing\n",
    "test_size = 0.15\n",
    "test_index = random.sample(range(len(dataset)), int(len(dataset) * test_size))\n",
    "test_set = [dataset[i] for i in test_index]\n",
    "training_set = [dataset[i] for i in range(len(dataset)) if i not in test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_na_glvq_cp = cglvq.CGLVQ(prototypes)\n",
    "sp_na_cp_hist = sp_na_glvq_cp.train(num_epochs, training_set, test_set, opt.conditional_probability,measure = \"test_measure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_na_glvq_ls = cglvq.CGLVQ(prototypes)\n",
    "sp_na_ls_hist = sp_na_glvq_ls.train(num_epochs, training_set, test_set, opt.loose_symmetry,measure = \"test_measure\")\n",
    "for epoch in sp_na_ls_hist:\n",
    "    print(epoch[\"prototypes\"][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without SP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with fourier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without fourier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal vs. Abnormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.load(\"dataset/feature/inputs_wo_ud_wo_sp.npy\")\n",
    "labels = np.load(\"dataset/label/labels_wo_ud.npy\")\n",
    "dataset_size = len(features)\n",
    "labels = np.where(labels == 0, labels, 1) # Divides the labels into normal(0) and abnormal(1)\n",
    "\n",
    "# Shuffle the dataset\n",
    "#permutation = np.random.permutation(dataset_size)\n",
    "#features = features[permutation]\n",
    "#labels = labels[permutation]\n",
    "\n",
    "unique, count = np.unique(labels, return_counts=True)\n",
    "dict(zip(unique, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prototype selection\n",
    "index_normal = np.where(labels == 0)\n",
    "index_normal = list(index_normal[0])\n",
    "index_abnormal = np.where(labels == 1)\n",
    "index_abnormal = list(index_abnormal[0])\n",
    "\n",
    "## Random prototype selection\n",
    "prototype_normal_index = random.sample(index_normal, num_prototypes)\n",
    "prototype_abnormal_index = random.sample(index_abnormal, num_prototypes)\n",
    "prototype_normal_index.sort(reverse=True)\n",
    "prototype_abnormal_index.sort(reverse=True)\n",
    "\n",
    "prototypes_index = prototype_normal_index + prototype_abnormal_index\n",
    "prototypes_index.sort(reverse=True)\n",
    "\n",
    "prototypes = [(features[i],labels[i]) for i in prototypes_index]\n",
    "\n",
    "## Remove prototypes from dataset\n",
    "features = np.delete(features, prototypes_index, axis=0)\n",
    "labels = np.delete(labels, prototypes_index, axis=0)    \n",
    "\n",
    "index_normal = np.where(labels == 0)\n",
    "index_normal = list(index_normal[0])\n",
    "index_abnormal = np.where(labels == 1)\n",
    "index_abnormal = list(index_abnormal[0])\n",
    "\n",
    "# Select equal amount of normal and abnormal data\n",
    "data_per_class = min(count) - num_prototypes\n",
    "index_normal_sample = random.sample(index_normal, data_per_class)\n",
    "dataset_index = index_normal_sample + index_abnormal\n",
    "\n",
    "#print(index_normal_sample)\n",
    "#print(index_abnormal)\n",
    "dataset_index.sort(reverse=True)\n",
    "\n",
    "dataset = [(features[i],labels[i]) for i in dataset_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split test, training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking 15% of the dataset for testing\n",
    "test_size = 0.15\n",
    "test_index = random.sample(range(len(dataset)), int(len(dataset) * test_size))\n",
    "test_set = [dataset[i] for i in test_index]\n",
    "training_set = [dataset[i] for i in range(len(dataset)) if i not in test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conditional Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_glvq_cp = cglvq.CGLVQ(prototypes)\n",
    "na_cp_hist = na_glvq_cp.train(num_epochs, training_set, test_set, opt.conditional_probability,measure = \"test_measure\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loose symmetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_glvq_ls = cglvq.CGLVQ(prototypes)\n",
    "na_ls_hist = na_glvq_ls.train(num_epochs, training_set, test_set, opt.loose_symmetry,measure = \"test_measure\")\n",
    "for epoch in sp_na_ls_hist:\n",
    "    print(epoch[\"prototypes\"][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GLVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import cognitive_GLVQ as cglvq\n",
    "import optimizer as opt\n",
    "import GLVQ as glvq\n",
    "\n",
    "prototypes = [ \n",
    "    (np.array([0.0,0.0,2]), np.array([1])), \n",
    "    (np.array([1.0,1.0,1]), np.array([0]))\n",
    "    ]\n",
    "\n",
    "dataset= [\n",
    "    (np.array([0.0,0.3,4]), np.array([0])),\n",
    "    (np.array([1.0,1.2,2]), np.array([1])),\n",
    "    (np.array([0.1,1.0,1]), np.array([1])),\n",
    "    (np.array([1.0,0.5,3]), np.array([0]))\n",
    "]\n",
    "\n",
    "\n",
    "#for i, data in enumerate(test):\n",
    "#    print(i, \" \",len(data[0]))\n",
    "\n",
    "#model = cglvq.CGLVQ(prototypes)\n",
    "#hist = model.train(100, dataset, dataset, opt.conditional_probability,measure = \"test_measure\")\n",
    "\n",
    "#for epoch in hist:\n",
    "#    print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import cognitive_GLVQ as cglvq\n",
    "import optimizer as opt\n",
    "import GLVQ as glvq\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "classes = {\n",
    "    \"Iris-setosa\": 0,\n",
    "    \"Iris-versicolor\": 1,\n",
    "    \"Iris-virginica\": 2,\n",
    "    \"target\": None\n",
    "}\n",
    " \n",
    "# opening the CSV file\n",
    "with open('dataset/iris_dataset.csv', mode ='r')as file:\n",
    "   \n",
    "  # reading the CSV file\n",
    "    csvFile = csv.reader(file)\n",
    " \n",
    "  # displaying the contents of the CSV file\n",
    "    features = []\n",
    "    labels = []\n",
    "    for row in csvFile:\n",
    "        feature = row[:-1]\n",
    "        label = [classes[row[-1]]]\n",
    "        features.append(feature)\n",
    "        labels.append(label)\n",
    "    features = np.array(features[1:], dtype=np.float32)\n",
    "    labels = np.array(labels[1:], dtype=np.int32)\n",
    "\n",
    "print (labels)\n",
    "#model_glvq = glvq.GLVQ(prototypes,learning_rate=0.3)\n",
    "#hist_glvq = model_glvq.train(100, dataset, dataset,measure = \"test_measure\")\n",
    "\n",
    "#print(hist_glvq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
